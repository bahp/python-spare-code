{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 02. Saving & Loading keras models\n\nThis script provides a hands-on demonstration of training an LSTM neural network\nfor time-series forecasting on the classic airline passenger dataset.\n\nIts primary focus is on illustrating three distinct methods for model\nserialization (saving), each suited for a different use case:\n\n1.  **Standard Method (`.keras`):** Simplest approach which saves a\n    complete model in a single binary file.\n2.  **Architecture + Weights (`.json` / `.h5`):** Separates the model's\n    human-readable structure from its binary weights, offering more flexibility.\n3.  **Readable Text Export (`.txt`):** A manual method for maximum portability,\n    ideal for secure environments or re-implementing the model in another language.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The script provides a mechanism for persisting the corresponding\n             scikit-learn scaler; however, this feature has not been formally\n             validated. Its performance on sophisticated scikit-learn preprocessing\n             pipelines has not been determined. The ability to reuse the identical\n             pipeline is a mandatory requirement for achieving a fully reproducible\n             prediction workflow</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd # pandas is needed for reading the URL\nimport tensorflow as tf\nimport jsonpickle\nimport jsonpickle.ext.numpy as jsonpickle_numpy\nfrom pathlib import Path\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.layers import Input\nfrom sklearn.preprocessing import MinMaxScaler\n\n# fix random seed for reproducibility\ntf.random.set_seed(7)\n\n# Configure jsonpickle to handle NumPy arrays\njsonpickle.set_preferred_backend('json')\njsonpickle_numpy.register_handlers()\n\n# -----------------------------------------------------------------\n# Helper methods\n# -----------------------------------------------------------------\ndef to_jsonpickle(obj, filename):\n    with open(filename, 'w') as f:\n        f.write(jsonpickle.encode(obj))\n\ndef from_jsonpickle(filename):\n    with open(filename, 'r') as f:\n        return jsonpickle.decode(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load and prepare the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n# Load and prepare data\n# --------------------------------------\nprint(\"Loading and preparing data from online URL...\")\n\n# URL for the raw airline passengers CSV file\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\ndf = pd.read_csv(url, usecols=[1], engine='python')\ndataset = df.values.astype('float32')\n\n# Normalize the dataset to the range [0, 1]\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(dataset)\n\n# Split into train and test sets\ntrain_size = int(len(scaled_data) * 0.67)\ntrain_data = scaled_data[0:train_size]\ntest_data = scaled_data[train_size:]\n\n# Prepare datasets for training and prediction\nlook_back = 1\nbatch_size = 1\n\n# Use the efficient Keras utility for the training dataset\ntrain_ds = tf.keras.utils.timeseries_dataset_from_array(\n    data=train_data[:-look_back],\n    targets=train_data[look_back:],\n    sequence_length=look_back,\n    batch_size=batch_size,\n)\n\n# For verification, we need a consistent NumPy array for test predictions\ndef create_numpy_sequences(data, look_back=1):\n    \"\"\"Creates X and Y NumPy arrays from time series data.\"\"\"\n    X, Y = [], []\n    for i in range(len(data) - look_back -1):\n        X.append(data[i:(i + look_back), 0])\n        Y.append(data[i + look_back, 0])\n    return np.array(X), np.array(Y)\n\ntestX, testY = create_numpy_sequences(test_data, look_back)\n# Reshape input to be [samples, time steps, features] for the LSTM model\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\nprint(f\"Data prepared. Test input shape: {testX.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets create and train our LSTM model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------\n# Model training\n# ---------------------------------------\n\ndef create_model(look_back=1):\n    \"\"\"Creates a compiled LSTM model.\"\"\"\n    model = Sequential([\n        Input(shape=(look_back, 1)),\n        LSTM(4),\n        Dense(1)\n    ])\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n# Create and train the model\nprint(\"\\nTraining LSTM model...\")\nmodel = create_model(look_back)\nmodel.fit(train_ds, epochs=100, verbose=0)\nprint(\"Model training complete.\")\n\n# Predictions from the original, in-memory model\noriginal_predictions = model.predict(testX)\nprint(f\"Made baseline predictions. Shape: {original_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save in memory using standard tensorflow/keras serialization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n# Example I: Standard Tensorflow/Keras serialization\n# --------------------------------------------------\nprint(\"\\n--- Example I: Standard Keras .save() / .load_model() ---\")\npath_output_keras = Path('./outputs/main02/keras_model')\nkeras_model_path = path_output_keras / 'model.keras'\npath_output_keras.mkdir(parents=True, exist_ok=True)\n\n# Save the model in the recommended .keras format\nmodel.save(keras_model_path)\nprint(f\"Model saved to {keras_model_path}\")\n\n# Load the model back\nloaded_keras_model = tf.keras.models.load_model(keras_model_path)\nprint(\"Model loaded successfully.\")\n\n# Verify predictions\nkeras_predictions = loaded_keras_model.predict(testX)\nare_equal_keras = np.array_equal(original_predictions, keras_predictions)\nprint(f\"==> [Keras] Are test set predictions equal? {are_equal_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save in memory using keras .json + weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------\n# Example II: Serialisation using keras JSON + weights\n# -----------------------------------------------------\nprint(\"\\n--- Example II: Serialization using Keras JSON (architecture) + H5 (weights) ---\")\n\nfrom tensorflow.keras.models import model_from_json\n\npath_output_jsonp = Path('./outputs/main02/jsonpickle_model')\npath_output_arch = path_output_jsonp / 'model_architecture.json'\npath_output_weights = path_output_jsonp / 'model_weights.weights.h5' # Use .h5 for weights\npath_output_json_scaler = path_output_jsonp / 'scaler.json'\npath_output_jsonp.mkdir(parents=True, exist_ok=True)\n\n# --- SAVING ---\n# 1. Save the model's architecture as a JSON string\nmodel_json = model.to_json()\nwith open(path_output_arch, \"w\") as json_file:\n    json_file.write(model_json)\n\n# 2. Save the model's weights\nmodel.save_weights(path_output_weights)\n\n# 3. Save the scaler (jsonpickle is fine for this simple object)\nto_jsonpickle(scaler, path_output_json_scaler)\nprint(\"Model architecture, weights, and scaler saved successfully.\")\n\n# --- LOADING ---\n# 1. Load the model architecture from the JSON file\nwith open(path_output_arch, 'r') as json_file:\n    loaded_model_json = json_file.read()\nloaded_model = model_from_json(loaded_model_json)\n\n# 2. Load the weights into the new model structure\nloaded_model.load_weights(path_output_weights)\nprint(\"Model loaded from architecture and weights.\")\n\n# 3. CRUCIAL: Compile the model after loading\n# The model must be compiled before you can use it.\nloaded_model.compile(loss='mean_squared_error', optimizer='adam')\n\n# --- VERIFYING ---\n# Now the loaded_model is a proper Keras model and has the .predict method\njson_predictions = loaded_model.predict(testX)\nare_equal_json = np.array_equal(original_predictions, json_predictions)\nprint(f\"==> [Keras JSON] Are test set predictions equal? {are_equal_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save in memory using readable .txt files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------\n# Example III: Serialization using readable .txt files\n# -----------------------------------------------------\nprint(\"\\n--- Example III: Serialization using readable .txt files ---\")\n\ndef to_txt(weights, path):\n    \"\"\"Saves model weights and their shapes to a directory of .txt files.\"\"\"\n    path = Path(path)\n    # Save shapes\n    shapes = [e.shape for e in weights]\n    with open(path / 'shapes.txt', 'w') as f:\n        f.write(str(shapes))\n    # Save weights\n    for i, e in enumerate(weights):\n        # Flatten array to save, will be reshaped on load\n        np.savetxt(path / f'weights_layer_{i}.txt', e.flatten())\n\ndef from_txt(path):\n    \"\"\"Loads weights from a directory of .txt files.\"\"\"\n    path = Path(path)\n    # Load shapes from shapes.txt\n    with open(path / 'shapes.txt', 'r') as f:\n        shapes = eval(f.read())\n    # Load weights and reshape\n    weights = []\n    for i, shape in enumerate(shapes):\n        w = np.loadtxt(path / f'weights_layer_{i}.txt')\n        weights.append(w.reshape(shape))\n    return weights\n\n# Create folder\npath_output_txt = Path('./outputs/main02/txt_model')\npath_output_txt.mkdir(parents=True, exist_ok=True)\n\n# Get weights from the original model and save them\noriginal_weights = model.get_weights()\nto_txt(original_weights, path=path_output_txt)\nprint(f\"Model weights saved to text files in {path_output_txt}\")\n\n# Load the weights from the text files\nloaded_txt_weights = from_txt(path=path_output_txt)\nprint(\"Weights loaded successfully from text files.\")\n\n# To use these weights, we must create an identical model structure\ntxt_model = create_model(look_back)\ntxt_model.set_weights(loaded_txt_weights)\nprint(\"New model created and weights have been set.\")\n\n# Verify predictions\ntxt_predictions = txt_model.predict(testX)\nare_equal_txt = np.array_equal(original_predictions, txt_predictions)\nprint(f\"==> [manual .txt] Are test set predictions equal? {are_equal_txt}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}