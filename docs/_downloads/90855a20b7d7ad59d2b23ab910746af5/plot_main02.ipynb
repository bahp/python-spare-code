{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 02. Explainers Across ML Classifiers\nThis script serves as a practical guide to applying the SHAP\nlibrary across a diverse set of machine learning algorithms.\nIt highlights a critical concept in model interpretability:\ndifferent model architectures require specific types of SHAP\nexplainers for accurate and efficient computation. \ud83e\udd16\n\nThe workflow includes:\n\n    - **Training Various Models:** A suite of classifiers from scikit-learn\n      and XGBoost are trained, including LogisticRegression, RandomForestClassifier,\n      SVC, and XGBClassifier.\n    - **Applying Appropriate Explainers:** The script demonstrates how to select\n      and use different explainers, primarily contrasting the model-agnostic\n      shap.KernelExplainer with the highly optimized explainer for tree-based\n      models.\n    - **Visual Comparison:** For each classifier, a SHAP summary plot is generated,\n      allowing for a side-by-side comparison of feature importances as interpreted\n      by each model.\n\nThis example is invaluable for understanding the practical nuances\nof using SHAP and for choosing the correct approach to explain the\npredictions of your specific machine learning model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Xgboost\nfrom xgboost import XGBClassifier\n\n# ----------------------------------------\n# Load data\n# ----------------------------------------\n# Seed\nseed = 0\n\n# Load dataset\nbunch = load_iris()\nbunch = load_breast_cancer()\nfeatures = list(bunch['feature_names'])\n\n# Create DataFrame\ndata = pd.DataFrame(data=np.c_[bunch['data'], bunch['target']],\n                    columns=features + ['target'])\n\n# Create X, y\nX = data[bunch['feature_names']]\ny = data['target']\n\n# Filter\nX = X.iloc[:500, :3]\ny = y.iloc[:500]\n\n\n# Split dataset\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, random_state=seed)\n\n\n# ----------------------------------------\n# Classifiers\n# ----------------------------------------\n# Train classifier\ngnb = GaussianNB()\nllr = LogisticRegression()\ndtc = DecisionTreeClassifier(random_state=seed)\nrfc = RandomForestClassifier(random_state=seed)\nxgb = XGBClassifier(\n    min_child_weight=0.005,\n    eta= 0.05, gamma= 0.2,\n    max_depth= 4,\n    n_estimators= 100)\nann = MLPClassifier()\nsvm = SVC(probability=True)\netc = ExtraTreesClassifier()\n\n# List\nclfs = [gnb, llr, dtc, rfc, xgb, ann, svm, etc]\n#clfs = [svm, dtc]\n\n# Fit\nfor clf in clfs:\n    clf.fit(X_train, y_train)\n\n# ----------------------------------------\n# Find shap values\n# ----------------------------------------\n# Possible explainers:\n#    - shap.DeepExplainer\n#    - shap.KernelExplainer\n#    - shap.TreeExplainer\n#    - shap.LinearExplainer\n#    - shap.Exact\n#    - shap.Explainer\n\n# Import\nimport shap\n\n# Initialise\nshap.initjs()\n\n\ndef predict_proba(x):\n    return clf.predict_proba(x)[:, 1]\n\n# Loop\nfor clf in clfs:\n\n    try:\n        # Show classifier\n        print(\"\\n\" + '-'*80)\n        print(\"Classifier: %s\" % clf)\n\n        \"\"\"\n        # Create shap explainer\n        if isinstance(clf,\n            (DecisionTreeClassifier,\n             ExtraTreesClassifier,\n             XGBClassifier)):\n            # Set Tree explainer\n            explainer = shap.TreeExplainer(clf)\n        elif isinstance(clf, LogisticRegression):\n            # Masker\n            masker = shap.maskers.Independent(X_train, max_samples=100)\n            # Set Linear explainer\n            #explainer = shap.LinearExplainer(predict_proba)#, masker)\n            explainer = shap.Explainer(predict_proba, masker)\n        elif isinstance(clf, int):\n            # Set NN explainer\n            explainer = shap.DeepExplainer(clf)\n        else:\n            # Works for [svc]\n            # If too many examples (pass aux to explainer).\n            aux = shap.sample(X_train, 100)\n            # Set generic kernel explainer\n            explainer = shap.KernelExplainer(predict_proba, aux)\n        \"\"\"\n\n        # Sample to speed up processing.\n        sample = shap.sample(X_train, 100)\n\n        if isinstance(clf, XGBClassifier):\n            # Works for [llr, dtc, etc, xgb]\n            explainer = shap.Explainer(clf, sample)\n        else:\n            # Works for all but [xgb]\n            explainer = shap.KernelExplainer(predict_proba, sample)\n\n        # Show kernel type\n        print(\"Kernel type: %s\" % type(explainer))\n\n        # Get shap values\n        #shap_values = explainer(X)\n        shap_values = explainer.shap_values(X_train)\n\n        print(shap_values)\n\n\n        # Show information\n        print(\"base value: %s\" % explainer.expected_value)\n        #print(\"shap_values: %s\" % str(shap_values.shape))\n\n        # Summary plot\n        plt.figure()\n        plot_summary = shap.summary_plot(\n            explainer.shap_values(X_train),\n            X_train, cmap='viridis', show=False\n        )\n\n        # Format\n        plt.title(clf.__class__.__name__)\n        plt.tight_layout()\n\n    except Exception as e:\n        print(\"Error: %s\" % e)\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}