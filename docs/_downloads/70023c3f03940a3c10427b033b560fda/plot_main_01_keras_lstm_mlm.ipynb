{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Serialization...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries\nimport json\nimport pandas\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\n# fix random seed for reproducibility\ntf.random.set_seed(7)\n\n# ---------------------------------------------------\n# Main example\n# ---------------------------------------------------\n# Copied from machine learning mastery\n\n# Path\npath = './data/passengers.csv'\n\n# Read data\ndf = pd.read_csv(path,\n    usecols=[1], engine='python')\n\n# Convert to numpy\ndataset = df.values\ndataset = dataset.astype('float32')\n\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))\n\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n# reshape into X=t and Y=t+1\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\ndef create_model(look_back=1):\n    \"\"\"\"\"\"\n    # create and fit the LSTM network\n    model = Sequential()\n    model.add(LSTM(4, input_shape=(1, look_back)))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    # Return\n    return model\n\n# create and fit the LSTM network\nmodel = create_model(look_back)\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n\"\"\"\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\n#plt.show()\n\"\"\"\n\n\n# ---------------------------------------\n# Save and load\n# ---------------------------------------\n# Jsonpickle\nimport jsonpickle\nimport jsonpickle.ext.numpy as jsonpickle_numpy\n\n# Configure jsonpickle\njsonpickle.set_preferred_backend('json')\njsonpickle_numpy.register_handlers()\n\n\ndef to_jsonpickle(obj, filename):\n    \"\"\"\"\"\"\n    with open(filename, 'w') as f:\n        f.write(jsonpickle.encode(obj))\n\n\ndef from_jsonpickle(filename):\n    \"\"\"\"\"\"\n    with open(filename, 'r') as f:\n        obj = jsonpickle.decode(f.read())\n    return obj\n\n\ndef to_txt(weights, path):\n    \"\"\"Converts a list of np.arrays to weights.\n\n    .. note: The np.arrays must be 2D or less.\n\n    Parameters\n    ----------\n    weights: list of np.arrays\n        The weights\n    path: str\n        The path to save the .txt files for each layer.\n\n    Returns\n    --------\n    \"\"\"\n    # Libraries\n    from pathlib import Path\n    path = Path(path)\n\n    # Warning if more than 2D\n\n    # Save shapes\n    shapes = [e.shape for e in weights]\n    with open(path / 'shapes.txt', 'w') as f:\n        f.write(str(shapes))\n\n    # Save weights\n    for i, e in enumerate(weights):\n        name = 'weights_layer_%s.txt' % i\n        np.savetxt(path / name, e ) #, fmt='%1.100e')\n\ndef from_txt(path):\n    \"\"\"Loads from a folder of txt files\n\n    Parameters\n    ----------\n    path: str\n        The path with the .txt files for each layer.\n\n    Returns\n    -------\n    \"\"\"\n    import glob\n    from pathlib import Path\n    path = Path(path)\n\n    # Load shapes\n    with open(path / 'shapes.txt', 'r') as f:\n        shapes = eval(f.read())\n\n    # Load weights\n    weights = []\n    for e in glob.glob(str(path / 'weights_*')):\n        weights.append(np.loadtxt(e))\n\n    # Reshape\n    reshaped = []\n    for s,w in zip(shapes, weights):\n        reshaped.append(np.reshape(w, s))\n\n    # Return\n    return reshaped\n\n\n\n\n\n\n# ------------------------------------------------------------\n# Example I: Serialization using jsonpickle\n# ------------------------------------------------------------\n\"\"\"Description:\n\nThe purpose of this example is to demonstrate that by \nemploying jsonpickle, we can store and retrieve the model, \nwhile ensuring that the predictions for the observations \nremain unchanged.\n\"\"\"\n\n# Paths\npath_output_json_scaler = './output/main01/scaler.json'\npath_output_json_model = './output/main01/model.json'\n\n# Create path if it does not exist\nPath(path_output_json_scaler) \\\n    .parent.mkdir(parents=True, exist_ok=True)\nPath(path_output_json_model) \\\n    .parent.mkdir(parents=True, exist_ok=True)\n\n# Save\nto_jsonpickle(scaler, path_output_json_scaler)\nto_jsonpickle(model, path_output_json_model)\n\n# Load model\njsonpickle_scaler = from_jsonpickle(path_output_json_scaler)\njsonpickle_model = from_jsonpickle(path_output_json_model)\n\n# Predict\njsonpickle_model_pred_test = jsonpickle_model.predict(testX)\n\n# Are the predictions equal?\nprint(\"==> [jsonpickle] Are test set predictions equal? %s\" %\n    np.array_equal(testPredict, jsonpickle_model_pred_test))\n\n\n\n\n\n# ------------------------------------------------------------\n# Example II: Serialization using readable txt\n# ------------------------------------------------------------\n\"\"\"Description:\n\nThe purpose of this example is to demonstrate that by \nemploying a manual approach with readable txt files, we \ncan store and retrieve the model, while ensuring that \nthe predictions for the observations remain unchanged.\n\"\"\"\n\n# Define path\npath_output_txt = './output/main01/txt'\n\n# Create path if it does not exist\nPath(path_output_txt).mkdir(parents=True, exist_ok=True)\n\n# Get weights from model\nw = jsonpickle_model.get_weights()\n\n# Save to txt\nto_txt(w, path=path_output_txt)\n\n# Load from txt\nweights = from_txt(path=path_output_txt)\n\n# Compare\n#for w1, a1 in zip(w, weights):\n#    print(np.array_equal(w1, a1))\n\n# Create model again from txt weights\ntxt_model = create_model(look_back=1)\ntxt_model.set_weights(weights)\ntxt_model_pred_test = txt_model.predict(testX)\n\n# Are the predictions equal?\nprint(\"==> [manualtxt] Are test set predictions equal? %s\" %\n    np.array_equal(testPredict, txt_model_pred_test))\n\n\n\n# ---------------------------------------\n#\n# ---------------------------------------\n# It seems that it does not work for the\n# scalers, only for a handful of models so\n# might need to do it manually too.\n\n\"\"\"\nimport sklearn_json as skljson\nskljson.to_json(new_scaler, './output/main01/test.json')\n\ndes_model = skljson.from_json('./output/main01/test.json')\nprint(des_model)\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}