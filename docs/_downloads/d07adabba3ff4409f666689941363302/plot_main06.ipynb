{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Shap - Main 06\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries\nimport shap\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\ntf.compat.v1.disable_v2_behavior()\n\n# --------------------------------------------\n# Create data\n# --------------------------------------------\n# Constants\nSAMPLES = 10\nTIMESTEPS = 10\nFEATURES = 5\n\n# .. note: Either perform a pre-processing step such as\n#          normalization or generate the features within\n#          the appropriate interval.\n# Create dataset\nx = np.random.randint(low=0, high=100,\n    size=(SAMPLES, TIMESTEPS, FEATURES))\ny = np.random.randint(low=0, high=2, size=SAMPLES).astype(float)\ni = np.vstack(np.dstack(np.indices((SAMPLES, TIMESTEPS))))\n\n# Create DataFrame\ndf = pd.DataFrame(\n    data=np.hstack((i, x.reshape((-1,FEATURES)))),\n    columns=['id', 't'] + ['f%s'%j for j in range(FEATURES)]\n)\n\n# Show\nprint(\"Shapes:\")\nprint(\"i: %s\" % str(i.shape))\nprint(\"y: %s\" % str(y.shape))\nprint(\"x: %s\" % str(x.shape))\n\nprint(\"\\nData (%s):\" % str(x.shape))\nprint(x)\n\nprint(\"\\nDataFrame (2D)\")\nprint(df)\n\n\n# --------------------------------------------\n# Model\n# --------------------------------------------\n# Libraries\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing import sequence\n\n# Create model\nmodel = Sequential()\n#model.add(Input(shape=(None, FEATURES)))\nmodel.add(\n    LSTM(\n        units=64,\n        return_sequences=False,\n        input_shape=(TIMESTEPS, FEATURES)\n    ))\n#model.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.run_eagerly = False\n\n# Load pre-trained weights\n\n# Display model summary\nprint(model.summary())\n\nmodel.save('outputs/model.h5')\n\n# Fit\nmodel.fit(x, y, epochs=16, batch_size=64)\n\n\n\n# --------------------------------------------\n# Compute and display SHAP values\n# --------------------------------------------\n# https://github.com/slundberg/shap/blob/master/shap/plots/_beeswarm.py\n\n# Use the training data for deep explainer => can use fewer instances\nexplainer = shap.DeepExplainer(model, x)\n# explain the the testing instances (can use fewer instanaces)\n# explaining each prediction requires 2 * background dataset size runs\nshap_values = explainer.shap_values(x)\n# init the JS visualization code\nshap.initjs()\n\nprint(shap_values[0].shape)\n\n#shap_values = explainer(x)\n\n\"\"\"\nshap.plots.beeswarm(shap_values,\n    max_display=12, order=shap.Explanation.abs.mean(0))\n\nimport matplotlib.pyplot as plt\nplt.show()\n\n\n\n\nimport sys\nsys.exit()\n\"\"\"\n\nshap_values_2D = shap_values[0].reshape(-1,x.shape[-1])\nx_2D = pd.DataFrame(\n    data=x.reshape(-1,x.shape[-1]),\n    columns=['f%s'%j for j in range(x.shape[-1])]\n)\n\n\n## SHAP for each time step\nNUM_STEPS = x.shape[1]\nNUM_FEATURES = x.shape[-1]\nlen_test_set = x_2D.shape[0]\n\n\"\"\"\n# step = 0\nfor step in range(NUM_STEPS):\n    indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]\n    shap_values_2D_step = shap_values_2D[indice]\n    x_test_2d_step = x_2D.iloc[indice]\n    print(\"_______ time step {} ___________\".format(step))\n    #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type=\"bar\")\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step)\n    print(\"\\n\")\n\"\"\"\n\n\nshap_values_2D_step = shap_values_2D[:, 1].reshape(-1, x.shape[1])\nx_test_2d_step = x_2D.iloc[:, 1].to_numpy().reshape(-1, x.shape[1])\nx_test_2d_step = pd.DataFrame(\n    x_test_2d_step, columns=['timestep %s'%j for j in range(x.shape[1])]\n)\n\nprint(x_test_2d_step)\n\nshap.summary_plot(shap_values_2D_step, x_test_2d_step, sort=False)\n\n\"\"\"\nfor step in range(NUM_STEPS):\n    indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]\n    shap_values_2D_step = shap_values_2D[indice]\n    x_test_2d_step = x_2D.iloc[indice]\n    print(\"_______ time step {} ___________\".format(step))\n    #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type=\"bar\")\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step)\n    print(\"\\n\")\n\"\"\"\nimport matplotlib.pyplot as plt\nplt.show()\n\n\"\"\"\n#y_pred = model.predict(x[:3, :, :])\n#print(y_pred)\n\n#background = x[np.random.choice(x.shape[0], 10, replace=False)]\nmasker = shap.maskers.Independent(data=x)\n# Get generic explainer\n#explainer = shap.KernelExplainer(model, background)\nexplainer = shap.KernelExplainer(model.predict, x, masker=masker)\n\n# Show kernel type\nprint(\"\\nKernel type: %s\" % type(explainer))\n\n# Get shap values\nshap_values = explainer.shap_values(x)\n\nprint(shap_values)\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}