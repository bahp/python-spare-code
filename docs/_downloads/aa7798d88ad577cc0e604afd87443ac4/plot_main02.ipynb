{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Shap - Main 02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Xgboost\nfrom xgboost import XGBClassifier\n\n# ----------------------------------------\n# Load data\n# ----------------------------------------\n# Seed\nseed = 0\n\n# Load dataset\nbunch = load_iris()\nbunch = load_breast_cancer()\nfeatures = list(bunch['feature_names'])\n\n# Create DataFrame\ndata = pd.DataFrame(data=np.c_[bunch['data'], bunch['target']],\n                    columns=features + ['target'])\n\n# Create X, y\nX = data[bunch['feature_names']]\ny = data['target']\n\n# Filter\nX = X.iloc[:500, :3]\ny = y.iloc[:500]\n\n\n# Split dataset\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, random_state=seed)\n\n\n# ----------------------------------------\n# Classifiers\n# ----------------------------------------\n# Train classifier\ngnb = GaussianNB()\nllr = LogisticRegression()\ndtc = DecisionTreeClassifier(random_state=seed)\nrfc = RandomForestClassifier(random_state=seed)\nxgb = XGBClassifier(\n    min_child_weight=0.005,\n    eta= 0.05, gamma= 0.2,\n    max_depth= 4,\n    n_estimators= 100)\nann = MLPClassifier()\nsvm = SVC(probability=True)\netc = ExtraTreesClassifier()\n\n# List\nclfs = [gnb, llr, dtc, rfc, xgb, ann, svm, etc]\n#clfs = [svm, dtc]\n\n# Fit\nfor clf in clfs:\n    clf.fit(X_train, y_train)\n\n# ----------------------------------------\n# Find shap values\n# ----------------------------------------\n# Possible explainers:\n#    - shap.DeepExplainer\n#    - shap.KernelExplainer\n#    - shap.TreeExplainer\n#    - shap.LinearExplainer\n#    - shap.Exact\n#    - shap.Explainer\n\n# Import\nimport shap\n\n# Initialise\nshap.initjs()\n\n\ndef predict_proba(x):\n    return clf.predict_proba(x)[:, 1]\n\n# Loop\nfor clf in clfs:\n\n    try:\n        # Show classifier\n        print(\"\\n\" + '-'*80)\n        print(\"Classifier: %s\" % clf)\n\n        \"\"\"\n        # Create shap explainer\n        if isinstance(clf,\n            (DecisionTreeClassifier,\n             ExtraTreesClassifier,\n             XGBClassifier)):\n            # Set Tree explainer\n            explainer = shap.TreeExplainer(clf)\n        elif isinstance(clf, LogisticRegression):\n            # Masker\n            masker = shap.maskers.Independent(X_train, max_samples=100)\n            # Set Linear explainer\n            #explainer = shap.LinearExplainer(predict_proba)#, masker)\n            explainer = shap.Explainer(predict_proba, masker)\n        elif isinstance(clf, int):\n            # Set NN explainer\n            explainer = shap.DeepExplainer(clf)\n        else:\n            # Works for [svc]\n            # If too many examples (pass aux to explainer).\n            aux = shap.sample(X_train, 100)\n            # Set generic kernel explainer\n            explainer = shap.KernelExplainer(predict_proba, aux)\n        \"\"\"\n\n        # Sample to speed up processing.\n        sample = shap.sample(X_train, 100)\n\n        if isinstance(clf, XGBClassifier):\n            # Works for [llr, dtc, etc, xgb]\n            explainer = shap.Explainer(clf, sample)\n        else:\n            # Works for all but [xgb]\n            explainer = shap.KernelExplainer(predict_proba, sample)\n\n        # Show kernel type\n        print(\"Kernel type: %s\" % type(explainer))\n\n        # Get shap values\n        #shap_values = explainer(X)\n        shap_values = explainer.shap_values(X_train)\n\n        print(shap_values)\n\n\n        # Show information\n        print(\"base value: %s\" % \\\n              explainer.expected_value)\n        #print(\"shap_values: %s\" % \\\n        #      str(shap_values.shape))\n\n        # Summary plot\n        plt.figure()\n        plot_summary = shap.summary_plot( \\\n            explainer.shap_values(X_train),\n            X_train, cmap='viridis',\n            show=False)\n\n        # Format\n        plt.title(clf.__class__.__name__)\n        plt.tight_layout()\n\n    except Exception as e:\n        print(\"Error: %s\" % e)\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}