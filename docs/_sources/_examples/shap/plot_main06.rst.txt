
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples/shap/plot_main06.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_shap_plot_main06.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_shap_plot_main06.py:


06. Basic example
=================

.. GENERATED FROM PYTHON SOURCE LINES 6-191



.. image-sg:: /_examples/shap/images/sphx_glr_plot_main06_001.png
   :alt: plot main06
   :srcset: /_examples/shap/images/sphx_glr_plot_main06_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    WARNING:tensorflow:From /Users/cbit/Desktop/repositories/environments/venv-py3109-python-spare-code/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    Shapes:
    i: (100, 2)
    y: (10,)
    x: (10, 10, 5)

    Data ((10, 10, 5)):
    [[[60 41 84 48 87]
      [37 14 28 83 23]
      [16 60 66  1 13]
      [99 34 14 22 88]
      [56 34 60 75 72]
      [55 28  7 50 93]
      [25 94 11 51 58]
      [15 88 90  1 75]
      [77 38 11 49  2]
      [56 37 72 95 57]]

     [[65 43 99 28 54]
      [34 39 46 61 28]
      [ 0  7 34 41 90]
      [95  1 66 19 13]
      [60 24 53 49  4]
      [31 66 81 39 28]
      [35 99 40 50 52]
      [35 66 13 26 97]
      [86 74 97 20  2]
      [ 4 53 77 93 72]]

     [[49 37 69 25 48]
      [16 26 63 89 94]
      [19 31 53 71 35]
      [21 48 70 89  8]
      [39  7 92 13 38]
      [64 76 16 55 28]
      [73 81 78 21 77]
      [79  5 95 62  6]
      [ 4 70 31 31 21]
      [18 36 26 44 88]]

     [[ 4 24 65 59 62]
      [85 99 40 67  3]
      [19 63 18 96 67]
      [31 21 60 20 92]
      [79 39 40 56 61]
      [57 93 45 85 65]
      [ 7 43 12 67 90]
      [30 97 78  8 12]
      [56 47 44 35 55]
      [16 43 20 10 14]]

     [[47 59 17 65 69]
      [57 37 56  4 58]
      [ 0 56 51 66 48]
      [60 53 32 58 75]
      [67 28 14 66 42]
      [54 72 70 83 51]
      [27 53 24 58 53]
      [64 48  9  0 16]
      [45 65 32 51 88]
      [ 3 49 36  4 44]]

     [[47 64  4 25 53]
      [37 10 25 95 94]
      [89 38 15 50 96]
      [82 86 45  8 61]
      [66 45 70 70 87]
      [99 60 12 97 37]
      [92 64 25 30 78]
      [44 65  4 79  7]
      [51 55 62 90 12]
      [69 19 25 41 75]]

     [[70  6 58 55 46]
      [46 28 52 71 41]
      [29 15 19 83 35]
      [ 0 27 69 21 21]
      [76 18 17 84 77]
      [44 21 70 40 62]
      [73 25 78 47 65]
      [97 71 25 36 99]
      [44 26 75 77 10]
      [71 86 76 79 42]]

     [[70 90  9 33 83]
      [ 3 82 60 29  7]
      [78 18 27 88 46]
      [98 22 91 24 65]
      [34 94 59 64 83]
      [57  3 17 10 79]
      [61 30 91 85 11]
      [40 99 50 31 23]
      [98 37 43 90 36]
      [53 70 50 26 61]]

     [[59 72  5  0 21]
      [41 98 48 95 72]
      [55 10 41 55  7]
      [17 94 48 29 69]
      [91 88 83 95 14]
      [40 48  2  4 64]
      [56 84 26 78 93]
      [28  3 21 38 72]
      [ 7 52 20 27 37]
      [59 11 82  7 14]]

     [[61 37  4 22 53]
      [23  8 81  1 45]
      [32 19 89 21 23]
      [90 39 42 45 65]
      [89 60 72 79 81]
      [22 45 71 54 32]
      [77 93 95 22  6]
      [64 26 55 24 13]
      [32 34 79 29 24]
      [74 59 47 61 94]]]

    DataFrame (2D)
        id  t  f0  f1  f2  f3  f4
    0    0  0  60  41  84  48  87
    1    0  1  37  14  28  83  23
    2    0  2  16  60  66   1  13
    3    0  3  99  34  14  22  88
    4    0  4  56  34  60  75  72
    ..  .. ..  ..  ..  ..  ..  ..
    95   9  5  22  45  71  54  32
    96   9  6  77  93  95  22   6
    97   9  7  64  26  55  24  13
    98   9  8  32  34  79  29  24
    99   9  9  74  59  47  61  94

    [100 rows x 7 columns]
    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm (LSTM)                 (None, 64)                17920     
                                                                 
     dense (Dense)               (None, 64)                4160      
                                                                 
     dense_1 (Dense)             (None, 1)                 65        
                                                                 
    =================================================================
    Total params: 22,145
    Trainable params: 22,145
    Non-trainable params: 0
    _________________________________________________________________
    None
    Train on 10 samples
    Epoch 1/16
    10/10 [==============================] - ETA: 0s - loss: 0.6971 - acc: 0.4000    10/10 [==============================] - 0s 14ms/sample - loss: 0.6971 - acc: 0.4000
    Epoch 2/16
    10/10 [==============================] - ETA: 0s - loss: 0.6634 - acc: 0.7000    10/10 [==============================] - 0s 418us/sample - loss: 0.6634 - acc: 0.7000
    Epoch 3/16
    10/10 [==============================] - ETA: 0s - loss: 0.6331 - acc: 0.8000    10/10 [==============================] - 0s 307us/sample - loss: 0.6331 - acc: 0.8000
    Epoch 4/16
    10/10 [==============================] - ETA: 0s - loss: 0.6055 - acc: 1.0000    10/10 [==============================] - 0s 253us/sample - loss: 0.6055 - acc: 1.0000
    Epoch 5/16
    10/10 [==============================] - ETA: 0s - loss: 0.5804 - acc: 1.0000    10/10 [==============================] - 0s 255us/sample - loss: 0.5804 - acc: 1.0000
    Epoch 6/16
    10/10 [==============================] - ETA: 0s - loss: 0.5568 - acc: 1.0000    10/10 [==============================] - 0s 252us/sample - loss: 0.5568 - acc: 1.0000
    Epoch 7/16
    10/10 [==============================] - ETA: 0s - loss: 0.5343 - acc: 1.0000    10/10 [==============================] - 0s 259us/sample - loss: 0.5343 - acc: 1.0000
    Epoch 8/16
    10/10 [==============================] - ETA: 0s - loss: 0.5124 - acc: 1.0000    10/10 [==============================] - 0s 264us/sample - loss: 0.5124 - acc: 1.0000
    Epoch 9/16
    10/10 [==============================] - ETA: 0s - loss: 0.4906 - acc: 1.0000    10/10 [==============================] - 0s 266us/sample - loss: 0.4906 - acc: 1.0000
    Epoch 10/16
    10/10 [==============================] - ETA: 0s - loss: 0.4686 - acc: 1.0000    10/10 [==============================] - 0s 278us/sample - loss: 0.4686 - acc: 1.0000
    Epoch 11/16
    10/10 [==============================] - ETA: 0s - loss: 0.4477 - acc: 1.0000    10/10 [==============================] - 0s 271us/sample - loss: 0.4477 - acc: 1.0000
    Epoch 12/16
    10/10 [==============================] - ETA: 0s - loss: 0.4295 - acc: 1.0000    10/10 [==============================] - 0s 329us/sample - loss: 0.4295 - acc: 1.0000
    Epoch 13/16
    10/10 [==============================] - ETA: 0s - loss: 0.4127 - acc: 1.0000    10/10 [==============================] - 0s 325us/sample - loss: 0.4127 - acc: 1.0000
    Epoch 14/16
    10/10 [==============================] - ETA: 0s - loss: 0.3972 - acc: 1.0000    10/10 [==============================] - 0s 291us/sample - loss: 0.3972 - acc: 1.0000
    Epoch 15/16
    10/10 [==============================] - ETA: 0s - loss: 0.3827 - acc: 1.0000    10/10 [==============================] - 0s 276us/sample - loss: 0.3827 - acc: 1.0000
    Epoch 16/16
    10/10 [==============================] - ETA: 0s - loss: 0.3685 - acc: 1.0000    10/10 [==============================] - 0s 290us/sample - loss: 0.3685 - acc: 1.0000
    keras is no longer supported, please use tf.keras instead.
    Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.
    WARNING:tensorflow:From /Users/cbit/Desktop/repositories/environments/venv-py3109-python-spare-code/lib/python3.10/site-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

    <IPython.core.display.HTML object>
    (10, 10, 5)
       timestep 0  timestep 1  timestep 2  timestep 3  timestep 4  timestep 5  timestep 6  timestep 7  timestep 8  timestep 9
    0          41          14          60          34          34          28          94          88          38          37
    1          43          39           7           1          24          66          99          66          74          53
    2          37          26          31          48           7          76          81           5          70          36
    3          24          99          63          21          39          93          43          97          47          43
    4          59          37          56          53          28          72          53          48          65          49
    5          64          10          38          86          45          60          64          65          55          19
    6           6          28          15          27          18          21          25          71          26          86
    7          90          82          18          22          94           3          30          99          37          70
    8          72          98          10          94          88          48          84           3          52          11
    9          37           8          19          39          60          45          93          26          34          59
    No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored

    '\n#y_pred = model.predict(x[:3, :, :])\n#print(y_pred)\n\n#background = x[np.random.choice(x.shape[0], 10, replace=False)]\nmasker = shap.maskers.Independent(data=x)\n# Get generic explainer\n#explainer = shap.KernelExplainer(model, background)\nexplainer = shap.KernelExplainer(model.predict, x, masker=masker)\n\n# Show kernel type\nprint("\nKernel type: %s" % type(explainer))\n\n# Get shap values\nshap_values = explainer.shap_values(x)\n\nprint(shap_values)\n'





|

.. code-block:: default
   :lineno-start: 6

    # Libraries
    import shap
    import numpy as np
    import pandas as pd

    import tensorflow as tf
    tf.compat.v1.disable_eager_execution()
    tf.compat.v1.disable_v2_behavior()

    # --------------------------------------------
    # Create data
    # --------------------------------------------
    # Constants
    SAMPLES = 10
    TIMESTEPS = 10
    FEATURES = 5

    # .. note: Either perform a pre-processing step such as
    #          normalization or generate the features within
    #          the appropriate interval.
    # Create dataset
    x = np.random.randint(low=0, high=100,
        size=(SAMPLES, TIMESTEPS, FEATURES))
    y = np.random.randint(low=0, high=2, size=SAMPLES).astype(float)
    i = np.vstack(np.dstack(np.indices((SAMPLES, TIMESTEPS))))

    # Create DataFrame
    df = pd.DataFrame(
        data=np.hstack((i, x.reshape((-1,FEATURES)))),
        columns=['id', 't'] + ['f%s'%j for j in range(FEATURES)]
    )

    # Show
    print("Shapes:")
    print("i: %s" % str(i.shape))
    print("y: %s" % str(y.shape))
    print("x: %s" % str(x.shape))

    print("\nData (%s):" % str(x.shape))
    print(x)

    print("\nDataFrame (2D)")
    print(df)


    # --------------------------------------------
    # Model
    # --------------------------------------------
    # Libraries
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.layers import Embedding
    from tensorflow.keras.preprocessing import sequence

    # Create model
    model = Sequential()
    #model.add(Input(shape=(None, FEATURES)))
    model.add(
        LSTM(
            units=64,
            return_sequences=False,
            input_shape=(TIMESTEPS, FEATURES)
        ))
    #model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    model.run_eagerly = False

    # Load pre-trained weights

    # Display model summary
    print(model.summary())

    model.save('outputs/model.h5')

    # Fit
    model.fit(x, y, epochs=16, batch_size=64)



    # --------------------------------------------
    # Compute and display SHAP values
    # --------------------------------------------
    # https://github.com/slundberg/shap/blob/master/shap/plots/_beeswarm.py

    # Use the training data for deep explainer => can use fewer instances
    explainer = shap.DeepExplainer(model, x)
    # explain the the testing instances (can use fewer instanaces)
    # explaining each prediction requires 2 * background dataset size runs
    shap_values = explainer.shap_values(x)
    # init the JS visualization code
    shap.initjs()

    print(shap_values[0].shape)

    #shap_values = explainer(x)

    """
    shap.plots.beeswarm(shap_values,
        max_display=12, order=shap.Explanation.abs.mean(0))

    import matplotlib.pyplot as plt
    plt.show()




    import sys
    sys.exit()
    """

    shap_values_2D = shap_values[0].reshape(-1,x.shape[-1])
    x_2D = pd.DataFrame(
        data=x.reshape(-1,x.shape[-1]),
        columns=['f%s'%j for j in range(x.shape[-1])]
    )


    ## SHAP for each time step
    NUM_STEPS = x.shape[1]
    NUM_FEATURES = x.shape[-1]
    len_test_set = x_2D.shape[0]

    """
    # step = 0
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """


    shap_values_2D_step = shap_values_2D[:, 1].reshape(-1, x.shape[1])
    x_test_2d_step = x_2D.iloc[:, 1].to_numpy().reshape(-1, x.shape[1])
    x_test_2d_step = pd.DataFrame(
        x_test_2d_step, columns=['timestep %s'%j for j in range(x.shape[1])]
    )

    print(x_test_2d_step)

    shap.summary_plot(shap_values_2D_step, x_test_2d_step, sort=False)

    """
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """
    import matplotlib.pyplot as plt
    plt.show()

    """
    #y_pred = model.predict(x[:3, :, :])
    #print(y_pred)

    #background = x[np.random.choice(x.shape[0], 10, replace=False)]
    masker = shap.maskers.Independent(data=x)
    # Get generic explainer
    #explainer = shap.KernelExplainer(model, background)
    explainer = shap.KernelExplainer(model.predict, x, masker=masker)

    # Show kernel type
    print("\nKernel type: %s" % type(explainer))

    # Get shap values
    shap_values = explainer.shap_values(x)

    print(shap_values)
    """


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  30.240 seconds)


.. _sphx_glr_download__examples_shap_plot_main06.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_main06.py <plot_main06.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_main06.ipynb <plot_main06.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
