
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\shap\plot_main06.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_shap_plot_main06.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_shap_plot_main06.py:


Shap - Main 06
==============

.. GENERATED FROM PYTHON SOURCE LINES 6-191



.. image-sg:: /_examples/shap/images/sphx_glr_plot_main06_001.png
   :alt: plot main06
   :srcset: /_examples/shap/images/sphx_glr_plot_main06_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    WARNING:tensorflow:From c:\users\kelda\desktop\repositories\virtualenvs\venv-py3790-psc\lib\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    Shapes:
    i: (100, 2)
    y: (10,)
    x: (10, 10, 5)

    Data ((10, 10, 5)):
    [[[39 27 63 85  4]
      [49 55 90 35 54]
      [ 5 75  5 98  6]
      [94 52 74 63 66]
      [ 8 74 49 90 98]
      [95 85 75 23 95]
      [58 96 84 68 44]
      [17 36  7 35 35]
      [10 17  4 14 36]
      [46 26 60 64 21]]

     [[90 45 14 37 25]
      [79 69 78 48 15]
      [73 52 23 79 78]
      [10 83 49 41 68]
      [75 14 30 77 70]
      [88 39 67 63 95]
      [80 58 82 85 99]
      [28 62  2 36 56]
      [59 33 94 14 17]
      [81 99 33 55 86]]

     [[69 20 87 67 93]
      [ 9 23 12 25 80]
      [27 41 84 41  2]
      [ 6  6  8 29 20]
      [63 77 11 64 13]
      [73 51 31 76 24]
      [42 36 77 99 86]
      [18 54 62  0 75]
      [27 53 60 16 44]
      [20 32 52 77 84]]

     [[38 74 79  7 46]
      [53 66  6 77 81]
      [52  3 97 62 30]
      [40 51 19 57 87]
      [40 80 58 70 26]
      [96  4 71 17 69]
      [53 49 68 33 85]
      [27 39  5 25 75]
      [84 33 99 75 79]
      [48 95 64 47 54]]

     [[47 85  3 68 21]
      [ 7 44 21 75 87]
      [27 17 60 88 36]
      [59 52 19 21 74]
      [64 41 58 30 19]
      [17 24 19  2 19]
      [56 42 53 51 88]
      [65 34 90  7 61]
      [79  4 37 57 99]
      [31  6 64 52 41]]

     [[79 62 87 14 50]
      [34 38 31 41 59]
      [54 77 50 64 77]
      [91 77 79 11 52]
      [ 4 89 40  6 92]
      [58 41 98 41 65]
      [ 6 82 31 89 68]
      [54 35 22 94 43]
      [ 4 73 92 27 93]
      [82 77 76 94 38]]

     [[71  3 48 19 29]
      [24 32 16 91 22]
      [45 18 71 56 25]
      [81 65 67 65  0]
      [ 8 42 95 54 18]
      [37 28 66 35 42]
      [23 47 92 80 67]
      [67 18 58 50 96]
      [92 80 37 20 90]
      [41 26 78 55 29]]

     [[22 55 42 49 60]
      [ 4 83 77 62 59]
      [81 75 83  1 45]
      [71  0 54  9 25]
      [72 79 44 45  8]
      [11 23 98 48 83]
      [49 26 56 41  2]
      [96 47 70 18 60]
      [ 9 45  5 86 67]
      [97 38 24 34 53]]

     [[82 24 95 30 69]
      [16 82 37  2 39]
      [31 61 48 27 54]
      [ 6 42 85 84 63]
      [98 85 76  9 97]
      [25  5 35 58 85]
      [84 85 16 30 23]
      [45 36 40 14 94]
      [16  6 73 23 97]
      [22 56 25  9 81]]

     [[ 2 45 27 49 23]
      [50 50 64 88  1]
      [ 9 74 10 28 42]
      [19 71 96 79 99]
      [96 95 53 32  6]
      [86 12 52 31 80]
      [28 24  8 77 62]
      [24 26 62 39 91]
      [16  4 79 20 70]
      [94 94 88 59 77]]]

    DataFrame (2D)
        id  t  f0  f1  f2  f3  f4
    0    0  0  39  27  63  85   4
    1    0  1  49  55  90  35  54
    2    0  2   5  75   5  98   6
    3    0  3  94  52  74  63  66
    4    0  4   8  74  49  90  98
    ..  .. ..  ..  ..  ..  ..  ..
    95   9  5  86  12  52  31  80
    96   9  6  28  24   8  77  62
    97   9  7  24  26  62  39  91
    98   9  8  16   4  79  20  70
    99   9  9  94  94  88  59  77

    [100 rows x 7 columns]
    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm (LSTM)                 (None, 64)                17920     
                                                                 
     dense (Dense)               (None, 64)                4160      
                                                                 
     dense_1 (Dense)             (None, 1)                 65        
                                                                 
    =================================================================
    Total params: 22,145
    Trainable params: 22,145
    Non-trainable params: 0
    _________________________________________________________________
    None
    Train on 10 samples
    Epoch 1/16

    10/10 [==============================] - ETA: 0s - loss: 0.7376 - acc: 0.4000
    10/10 [==============================] - 0s 18ms/sample - loss: 0.7376 - acc: 0.4000
    Epoch 2/16

    10/10 [==============================] - ETA: 0s - loss: 0.6931 - acc: 0.6000
    10/10 [==============================] - 0s 397us/sample - loss: 0.6931 - acc: 0.6000
    Epoch 3/16

    10/10 [==============================] - ETA: 0s - loss: 0.6556 - acc: 0.7000
    10/10 [==============================] - 0s 297us/sample - loss: 0.6556 - acc: 0.7000
    Epoch 4/16

    10/10 [==============================] - ETA: 0s - loss: 0.6235 - acc: 0.9000
    10/10 [==============================] - 0s 400us/sample - loss: 0.6235 - acc: 0.9000
    Epoch 5/16

    10/10 [==============================] - ETA: 0s - loss: 0.5937 - acc: 0.9000
    10/10 [==============================] - 0s 299us/sample - loss: 0.5937 - acc: 0.9000
    Epoch 6/16

    10/10 [==============================] - ETA: 0s - loss: 0.5670 - acc: 0.9000
    10/10 [==============================] - 0s 300us/sample - loss: 0.5670 - acc: 0.9000
    Epoch 7/16

    10/10 [==============================] - ETA: 0s - loss: 0.5434 - acc: 0.9000
    10/10 [==============================] - 0s 397us/sample - loss: 0.5434 - acc: 0.9000
    Epoch 8/16

    10/10 [==============================] - ETA: 0s - loss: 0.5223 - acc: 0.9000
    10/10 [==============================] - 0s 400us/sample - loss: 0.5223 - acc: 0.9000
    Epoch 9/16

    10/10 [==============================] - ETA: 0s - loss: 0.5019 - acc: 0.9000
    10/10 [==============================] - 0s 400us/sample - loss: 0.5019 - acc: 0.9000
    Epoch 10/16

    10/10 [==============================] - ETA: 0s - loss: 0.4822 - acc: 0.9000
    10/10 [==============================] - 0s 400us/sample - loss: 0.4822 - acc: 0.9000
    Epoch 11/16

    10/10 [==============================] - ETA: 0s - loss: 0.4626 - acc: 0.9000
    10/10 [==============================] - 0s 300us/sample - loss: 0.4626 - acc: 0.9000
    Epoch 12/16

    10/10 [==============================] - ETA: 0s - loss: 0.4429 - acc: 1.0000
    10/10 [==============================] - 0s 300us/sample - loss: 0.4429 - acc: 1.0000
    Epoch 13/16

    10/10 [==============================] - ETA: 0s - loss: 0.4235 - acc: 1.0000
    10/10 [==============================] - 0s 300us/sample - loss: 0.4235 - acc: 1.0000
    Epoch 14/16

    10/10 [==============================] - ETA: 0s - loss: 0.4050 - acc: 1.0000
    10/10 [==============================] - 0s 201us/sample - loss: 0.4050 - acc: 1.0000
    Epoch 15/16

    10/10 [==============================] - ETA: 0s - loss: 0.3886 - acc: 1.0000
    10/10 [==============================] - 0s 297us/sample - loss: 0.3886 - acc: 1.0000
    Epoch 16/16

    10/10 [==============================] - ETA: 0s - loss: 0.3733 - acc: 1.0000
    10/10 [==============================] - 0s 403us/sample - loss: 0.3733 - acc: 1.0000
    WARNING:tensorflow:From c:\users\kelda\desktop\repositories\virtualenvs\venv-py3790-psc\lib\site-packages\shap\explainers\tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

    <IPython.core.display.HTML object>
    (10, 10, 5)
       timestep 0  timestep 1  timestep 2  timestep 3  timestep 4  timestep 5  timestep 6  timestep 7  timestep 8  timestep 9
    0          27          55          75          52          74          85          96          36          17          26
    1          45          69          52          83          14          39          58          62          33          99
    2          20          23          41           6          77          51          36          54          53          32
    3          74          66           3          51          80           4          49          39          33          95
    4          85          44          17          52          41          24          42          34           4           6
    5          62          38          77          77          89          41          82          35          73          77
    6           3          32          18          65          42          28          47          18          80          26
    7          55          83          75           0          79          23          26          47          45          38
    8          24          82          61          42          85           5          85          36           6          56
    9          45          50          74          71          95          12          24          26           4          94

    '\n#y_pred = model.predict(x[:3, :, :])\n#print(y_pred)\n\n#background = x[np.random.choice(x.shape[0], 10, replace=False)]\nmasker = shap.maskers.Independent(data=x)\n# Get generic explainer\n#explainer = shap.KernelExplainer(model, background)\nexplainer = shap.KernelExplainer(model.predict, x, masker=masker)\n\n# Show kernel type\nprint("\nKernel type: %s" % type(explainer))\n\n# Get shap values\nshap_values = explainer.shap_values(x)\n\nprint(shap_values)\n'





|

.. code-block:: default
   :lineno-start: 6

    # Libraries
    import shap
    import numpy as np
    import pandas as pd

    import tensorflow as tf
    tf.compat.v1.disable_eager_execution()
    tf.compat.v1.disable_v2_behavior()

    # --------------------------------------------
    # Create data
    # --------------------------------------------
    # Constants
    SAMPLES = 10
    TIMESTEPS = 10
    FEATURES = 5

    # .. note: Either perform a pre-processing step such as
    #          normalization or generate the features within
    #          the appropriate interval.
    # Create dataset
    x = np.random.randint(low=0, high=100,
        size=(SAMPLES, TIMESTEPS, FEATURES))
    y = np.random.randint(low=0, high=2, size=SAMPLES).astype(float)
    i = np.vstack(np.dstack(np.indices((SAMPLES, TIMESTEPS))))

    # Create DataFrame
    df = pd.DataFrame(
        data=np.hstack((i, x.reshape((-1,FEATURES)))),
        columns=['id', 't'] + ['f%s'%j for j in range(FEATURES)]
    )

    # Show
    print("Shapes:")
    print("i: %s" % str(i.shape))
    print("y: %s" % str(y.shape))
    print("x: %s" % str(x.shape))

    print("\nData (%s):" % str(x.shape))
    print(x)

    print("\nDataFrame (2D)")
    print(df)


    # --------------------------------------------
    # Model
    # --------------------------------------------
    # Libraries
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.layers import Embedding
    from tensorflow.keras.preprocessing import sequence

    # Create model
    model = Sequential()
    #model.add(Input(shape=(None, FEATURES)))
    model.add(
        LSTM(
            units=64,
            return_sequences=False,
            input_shape=(TIMESTEPS, FEATURES)
        ))
    #model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    model.run_eagerly = False

    # Load pre-trained weights

    # Display model summary
    print(model.summary())

    model.save('outputs/model.h5')

    # Fit
    model.fit(x, y, epochs=16, batch_size=64)



    # --------------------------------------------
    # Compute and display SHAP values
    # --------------------------------------------
    # https://github.com/slundberg/shap/blob/master/shap/plots/_beeswarm.py

    # Use the training data for deep explainer => can use fewer instances
    explainer = shap.DeepExplainer(model, x)
    # explain the the testing instances (can use fewer instanaces)
    # explaining each prediction requires 2 * background dataset size runs
    shap_values = explainer.shap_values(x)
    # init the JS visualization code
    shap.initjs()

    print(shap_values[0].shape)

    #shap_values = explainer(x)

    """
    shap.plots.beeswarm(shap_values,
        max_display=12, order=shap.Explanation.abs.mean(0))

    import matplotlib.pyplot as plt
    plt.show()




    import sys
    sys.exit()
    """

    shap_values_2D = shap_values[0].reshape(-1,x.shape[-1])
    x_2D = pd.DataFrame(
        data=x.reshape(-1,x.shape[-1]),
        columns=['f%s'%j for j in range(x.shape[-1])]
    )


    ## SHAP for each time step
    NUM_STEPS = x.shape[1]
    NUM_FEATURES = x.shape[-1]
    len_test_set = x_2D.shape[0]

    """
    # step = 0
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """


    shap_values_2D_step = shap_values_2D[:, 1].reshape(-1, x.shape[1])
    x_test_2d_step = x_2D.iloc[:, 1].to_numpy().reshape(-1, x.shape[1])
    x_test_2d_step = pd.DataFrame(
        x_test_2d_step, columns=['timestep %s'%j for j in range(x.shape[1])]
    )

    print(x_test_2d_step)

    shap.summary_plot(shap_values_2D_step, x_test_2d_step, sort=False)

    """
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """
    import matplotlib.pyplot as plt
    plt.show()

    """
    #y_pred = model.predict(x[:3, :, :])
    #print(y_pred)

    #background = x[np.random.choice(x.shape[0], 10, replace=False)]
    masker = shap.maskers.Independent(data=x)
    # Get generic explainer
    #explainer = shap.KernelExplainer(model, background)
    explainer = shap.KernelExplainer(model.predict, x, masker=masker)

    # Show kernel type
    print("\nKernel type: %s" % type(explainer))

    # Get shap values
    shap_values = explainer.shap_values(x)

    print(shap_values)
    """


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  4.049 seconds)


.. _sphx_glr_download__examples_shap_plot_main06.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_main06.py <plot_main06.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_main06.ipynb <plot_main06.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
