
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\shap\plot_main06.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_shap_plot_main06.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_shap_plot_main06.py:


Shap - Main 06
==============

.. GENERATED FROM PYTHON SOURCE LINES 6-191



.. image:: /_examples/shap/images/sphx_glr_plot_main06_001.png
    :alt: plot main06
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Shapes:
    i: (100, 2)
    y: (10,)
    x: (10, 10, 5)

    Data ((10, 10, 5)):
    [[[94 69 81 80 62]
      [44 77 96 73  6]
      [57 15 99 82 53]
      [94 42 57 42 90]
      [36 52 13 39 89]
      [43 57 73 80 62]
      [54 77 17 86 14]
      [93 88 37 11 18]
      [57 76 66 93 87]
      [18 62 45  0  4]]

     [[34 34 62 64 46]
      [18 11 38 14 86]
      [64 78 42  0 82]
      [84 41 25 42 67]
      [58 13 46 86 48]
      [84 37  9 29 66]
      [75 63 41 25 80]
      [21 49 52 90 12]
      [51 19 29 47 56]
      [86 67 24 10 31]]

     [[94 71 95 44 19]
      [78 18 47 91 27]
      [76 76 74 58 15]
      [93 38 66 11  5]
      [20 48 44 74  5]
      [42 20 23 26  5]
      [18 79 66 10 84]
      [99 93 47 45 23]
      [78 98  3 18 48]
      [80  1 32 52 84]]

     [[73 85 77 60 87]
      [62 62 49 15 57]
      [ 3  5 97 99 22]
      [31 31 68 94 88]
      [ 3 93 30 87 70]
      [42  1 52 42 44]
      [24 59 11 63 13]
      [76 57 59 92 82]
      [65 81 33 61 78]
      [ 4 58 17 84 21]]

     [[83 55 27 83 19]
      [41 59 66  9  8]
      [54 27 18 37  4]
      [30 13 53 13 85]
      [42 27 13 49 62]
      [ 5 88 85 54 97]
      [89  5 16 28 45]
      [86 29 68  3 17]
      [44 41 10 65 39]
      [66 77 87  8 99]]

     [[70 33 61 51 77]
      [72  8 15 52 60]
      [37 37 85 95 74]
      [46 85 29 54  2]
      [91 71 10 12 61]
      [99 16 79 44 84]
      [97 25 35 51 47]
      [32 53 91 24 95]
      [37 86 29 86 86]
      [61 98 21  2 24]]

     [[70 20 55 32 62]
      [55 72  9 85 76]
      [37 81 38 35 17]
      [96 78 98 85 26]
      [83 32 98 71 64]
      [ 7 11 57 26 69]
      [71  2 58 69 56]
      [30 73 92 53 69]
      [53 94  5 67 86]
      [67 27 28 63 94]]

     [[86 43 44 31 82]
      [ 5 12 40 45 31]
      [37 75 30 80 52]
      [62 17 39 97 94]
      [35  3 66 53 77]
      [32 44 10 76 60]
      [84 26  7  1 44]
      [10 69 49 86 60]
      [14 56 61 45 53]
      [14 56 21 33 73]]

     [[22 16 12 77 46]
      [20 34 78 26  7]
      [40 28 38 13 34]
      [46 88 72 13 30]
      [ 1 87  2 14 59]
      [70  6 91  7  6]
      [67 22 38 65 66]
      [59 72 98 78 29]
      [10 15 84 25 78]
      [87 18 99 13  1]]

     [[ 5 56 38  5 92]
      [79 83 98 33 93]
      [58 64 20 73 45]
      [66 88 60 86 68]
      [37 99 60 38 12]
      [51 30 77 81 42]
      [72 39 18 63 31]
      [60 41 68 43 44]
      [62 60 28  5 52]
      [34 79 15 77 24]]]

    DataFrame (2D)
        id  t  f0  f1  f2  f3  f4
    0    0  0  94  69  81  80  62
    1    0  1  44  77  96  73   6
    2    0  2  57  15  99  82  53
    3    0  3  94  42  57  42  90
    4    0  4  36  52  13  39  89
    ..  .. ..  ..  ..  ..  ..  ..
    95   9  5  51  30  77  81  42
    96   9  6  72  39  18  63  31
    97   9  7  60  41  68  43  44
    98   9  8  62  60  28   5  52
    99   9  9  34  79  15  77  24

    [100 rows x 7 columns]
    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm (LSTM)                 (None, 64)                17920     
                                                                 
     dense (Dense)               (None, 64)                4160      
                                                                 
     dense_1 (Dense)             (None, 1)                 65        
                                                                 
    =================================================================
    Total params: 22,145
    Trainable params: 22,145
    Non-trainable params: 0
    _________________________________________________________________
    None
    Train on 10 samples
    Epoch 1/16
    10/10 [==============================] - ETA: 0s - loss: 0.7149 - acc: 0.5000    10/10 [==============================] - 0s 15ms/sample - loss: 0.7149 - acc: 0.5000
    Epoch 2/16
    10/10 [==============================] - ETA: 0s - loss: 0.6581 - acc: 0.7000    10/10 [==============================] - 0s 298us/sample - loss: 0.6581 - acc: 0.7000
    Epoch 3/16
    10/10 [==============================] - ETA: 0s - loss: 0.6099 - acc: 0.7000    10/10 [==============================] - 0s 297us/sample - loss: 0.6099 - acc: 0.7000
    Epoch 4/16
    10/10 [==============================] - ETA: 0s - loss: 0.5652 - acc: 0.8000    10/10 [==============================] - 0s 403us/sample - loss: 0.5652 - acc: 0.8000
    Epoch 5/16
    10/10 [==============================] - ETA: 0s - loss: 0.5307 - acc: 0.8000    10/10 [==============================] - 0s 397us/sample - loss: 0.5307 - acc: 0.8000
    Epoch 6/16
    10/10 [==============================] - ETA: 0s - loss: 0.5025 - acc: 0.8000    10/10 [==============================] - 0s 299us/sample - loss: 0.5025 - acc: 0.8000
    Epoch 7/16
    10/10 [==============================] - ETA: 0s - loss: 0.4779 - acc: 0.8000    10/10 [==============================] - 0s 399us/sample - loss: 0.4779 - acc: 0.8000
    Epoch 8/16
    10/10 [==============================] - ETA: 0s - loss: 0.4544 - acc: 0.8000    10/10 [==============================] - 0s 297us/sample - loss: 0.4544 - acc: 0.8000
    Epoch 9/16
    10/10 [==============================] - ETA: 0s - loss: 0.4324 - acc: 0.8000    10/10 [==============================] - 0s 400us/sample - loss: 0.4324 - acc: 0.8000
    Epoch 10/16
    10/10 [==============================] - ETA: 0s - loss: 0.4099 - acc: 0.8000    10/10 [==============================] - 0s 361us/sample - loss: 0.4099 - acc: 0.8000
    Epoch 11/16
    10/10 [==============================] - ETA: 0s - loss: 0.3884 - acc: 0.9000    10/10 [==============================] - 0s 347us/sample - loss: 0.3884 - acc: 0.9000
    Epoch 12/16
    10/10 [==============================] - ETA: 0s - loss: 0.3681 - acc: 0.9000    10/10 [==============================] - 0s 297us/sample - loss: 0.3681 - acc: 0.9000
    Epoch 13/16
    10/10 [==============================] - ETA: 0s - loss: 0.3489 - acc: 0.9000    10/10 [==============================] - 0s 297us/sample - loss: 0.3489 - acc: 0.9000
    Epoch 14/16
    10/10 [==============================] - ETA: 0s - loss: 0.3303 - acc: 0.9000    10/10 [==============================] - 0s 300us/sample - loss: 0.3303 - acc: 0.9000
    Epoch 15/16
    10/10 [==============================] - ETA: 0s - loss: 0.3122 - acc: 0.9000    10/10 [==============================] - 0s 303us/sample - loss: 0.3122 - acc: 0.9000
    Epoch 16/16
    10/10 [==============================] - ETA: 0s - loss: 0.2943 - acc: 0.9000    10/10 [==============================] - 0s 297us/sample - loss: 0.2943 - acc: 0.9000
    <IPython.core.display.HTML object>
    (10, 10, 5)
       timestep 0  timestep 1  timestep 2  timestep 3  timestep 4  timestep 5  timestep 6  timestep 7  timestep 8  timestep 9
    0          69          77          15          42          52          57          77          88          76          62
    1          34          11          78          41          13          37          63          49          19          67
    2          71          18          76          38          48          20          79          93          98           1
    3          85          62           5          31          93           1          59          57          81          58
    4          55          59          27          13          27          88           5          29          41          77
    5          33           8          37          85          71          16          25          53          86          98
    6          20          72          81          78          32          11           2          73          94          27
    7          43          12          75          17           3          44          26          69          56          56
    8          16          34          28          88          87           6          22          72          15          18
    9          56          83          64          88          99          30          39          41          60          79

    '\n#y_pred = model.predict(x[:3, :, :])\n#print(y_pred)\n\n#background = x[np.random.choice(x.shape[0], 10, replace=False)]\nmasker = shap.maskers.Independent(data=x)\n# Get generic explainer\n#explainer = shap.KernelExplainer(model, background)\nexplainer = shap.KernelExplainer(model.predict, x, masker=masker)\n\n# Show kernel type\nprint("\nKernel type: %s" % type(explainer))\n\n# Get shap values\nshap_values = explainer.shap_values(x)\n\nprint(shap_values)\n'





|

.. code-block:: default
   :lineno-start: 6

    # Libraries
    import shap
    import numpy as np
    import pandas as pd

    import tensorflow as tf
    tf.compat.v1.disable_eager_execution()
    tf.compat.v1.disable_v2_behavior()

    # --------------------------------------------
    # Create data
    # --------------------------------------------
    # Constants
    SAMPLES = 10
    TIMESTEPS = 10
    FEATURES = 5

    # .. note: Either perform a pre-processing step such as
    #          normalization or generate the features within
    #          the appropriate interval.
    # Create dataset
    x = np.random.randint(low=0, high=100,
        size=(SAMPLES, TIMESTEPS, FEATURES))
    y = np.random.randint(low=0, high=2, size=SAMPLES).astype(float)
    i = np.vstack(np.dstack(np.indices((SAMPLES, TIMESTEPS))))

    # Create DataFrame
    df = pd.DataFrame(
        data=np.hstack((i, x.reshape((-1,FEATURES)))),
        columns=['id', 't'] + ['f%s'%j for j in range(FEATURES)]
    )

    # Show
    print("Shapes:")
    print("i: %s" % str(i.shape))
    print("y: %s" % str(y.shape))
    print("x: %s" % str(x.shape))

    print("\nData (%s):" % str(x.shape))
    print(x)

    print("\nDataFrame (2D)")
    print(df)


    # --------------------------------------------
    # Model
    # --------------------------------------------
    # Libraries
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.layers import Embedding
    from tensorflow.keras.preprocessing import sequence

    # Create model
    model = Sequential()
    #model.add(Input(shape=(None, FEATURES)))
    model.add(
        LSTM(
            units=64,
            return_sequences=False,
            input_shape=(TIMESTEPS, FEATURES)
        ))
    #model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    model.run_eagerly = False

    # Load pre-trained weights

    # Display model summary
    print(model.summary())

    model.save('outputs/model.h5')

    # Fit
    model.fit(x, y, epochs=16, batch_size=64)



    # --------------------------------------------
    # Compute and display SHAP values
    # --------------------------------------------
    # https://github.com/slundberg/shap/blob/master/shap/plots/_beeswarm.py

    # Use the training data for deep explainer => can use fewer instances
    explainer = shap.DeepExplainer(model, x)
    # explain the the testing instances (can use fewer instanaces)
    # explaining each prediction requires 2 * background dataset size runs
    shap_values = explainer.shap_values(x)
    # init the JS visualization code
    shap.initjs()

    print(shap_values[0].shape)

    #shap_values = explainer(x)

    """
    shap.plots.beeswarm(shap_values,
        max_display=12, order=shap.Explanation.abs.mean(0))

    import matplotlib.pyplot as plt
    plt.show()




    import sys
    sys.exit()
    """

    shap_values_2D = shap_values[0].reshape(-1,x.shape[-1])
    x_2D = pd.DataFrame(
        data=x.reshape(-1,x.shape[-1]),
        columns=['f%s'%j for j in range(x.shape[-1])]
    )


    ## SHAP for each time step
    NUM_STEPS = x.shape[1]
    NUM_FEATURES = x.shape[-1]
    len_test_set = x_2D.shape[0]

    """
    # step = 0
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """


    shap_values_2D_step = shap_values_2D[:, 1].reshape(-1, x.shape[1])
    x_test_2d_step = x_2D.iloc[:, 1].to_numpy().reshape(-1, x.shape[1])
    x_test_2d_step = pd.DataFrame(
        x_test_2d_step, columns=['timestep %s'%j for j in range(x.shape[1])]
    )

    print(x_test_2d_step)

    shap.summary_plot(shap_values_2D_step, x_test_2d_step, sort=False)

    """
    for step in range(NUM_STEPS):
        indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]
        shap_values_2D_step = shap_values_2D[indice]
        x_test_2d_step = x_2D.iloc[indice]
        print("_______ time step {} ___________".format(step))
        #shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type="bar")
        shap.summary_plot(shap_values_2D_step, x_test_2d_step)
        print("\n")
    """
    import matplotlib.pyplot as plt
    plt.show()

    """
    #y_pred = model.predict(x[:3, :, :])
    #print(y_pred)

    #background = x[np.random.choice(x.shape[0], 10, replace=False)]
    masker = shap.maskers.Independent(data=x)
    # Get generic explainer
    #explainer = shap.KernelExplainer(model, background)
    explainer = shap.KernelExplainer(model.predict, x, masker=masker)

    # Show kernel type
    print("\nKernel type: %s" % type(explainer))

    # Get shap values
    shap_values = explainer.shap_values(x)

    print(shap_values)
    """


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.663 seconds)


.. _sphx_glr_download__examples_shap_plot_main06.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_main06.py <plot_main06.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_main06.ipynb <plot_main06.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
