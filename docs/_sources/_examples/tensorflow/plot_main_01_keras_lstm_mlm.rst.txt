
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\tensorflow\plot_main_01_keras_lstm_mlm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_tensorflow_plot_main_01_keras_lstm_mlm.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_tensorflow_plot_main_01_keras_lstm_mlm.py:


Serialization...
=====================

.. GENERATED FROM PYTHON SOURCE LINES 7-301




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    96 48
    Epoch 1/100
    94/94 - 1s - loss: 0.0533 - 1s/epoch - 15ms/step
    Epoch 2/100
    94/94 - 0s - loss: 0.0275 - 110ms/epoch - 1ms/step
    Epoch 3/100
    94/94 - 0s - loss: 0.0191 - 98ms/epoch - 1ms/step
    Epoch 4/100
    94/94 - 0s - loss: 0.0171 - 99ms/epoch - 1ms/step
    Epoch 5/100
    94/94 - 0s - loss: 0.0161 - 100ms/epoch - 1ms/step
    Epoch 6/100
    94/94 - 0s - loss: 0.0150 - 97ms/epoch - 1ms/step
    Epoch 7/100
    94/94 - 0s - loss: 0.0141 - 97ms/epoch - 1ms/step
    Epoch 8/100
    94/94 - 0s - loss: 0.0131 - 104ms/epoch - 1ms/step
    Epoch 9/100
    94/94 - 0s - loss: 0.0123 - 100ms/epoch - 1ms/step
    Epoch 10/100
    94/94 - 0s - loss: 0.0112 - 100ms/epoch - 1ms/step
    Epoch 11/100
    94/94 - 0s - loss: 0.0103 - 100ms/epoch - 1ms/step
    Epoch 12/100
    94/94 - 0s - loss: 0.0093 - 100ms/epoch - 1ms/step
    Epoch 13/100
    94/94 - 0s - loss: 0.0084 - 100ms/epoch - 1ms/step
    Epoch 14/100
    94/94 - 0s - loss: 0.0075 - 100ms/epoch - 1ms/step
    Epoch 15/100
    94/94 - 0s - loss: 0.0067 - 101ms/epoch - 1ms/step
    Epoch 16/100
    94/94 - 0s - loss: 0.0060 - 102ms/epoch - 1ms/step
    Epoch 17/100
    94/94 - 0s - loss: 0.0052 - 96ms/epoch - 1ms/step
    Epoch 18/100
    94/94 - 0s - loss: 0.0045 - 98ms/epoch - 1ms/step
    Epoch 19/100
    94/94 - 0s - loss: 0.0041 - 98ms/epoch - 1ms/step
    Epoch 20/100
    94/94 - 0s - loss: 0.0036 - 98ms/epoch - 1ms/step
    Epoch 21/100
    94/94 - 0s - loss: 0.0032 - 98ms/epoch - 1ms/step
    Epoch 22/100
    94/94 - 0s - loss: 0.0029 - 103ms/epoch - 1ms/step
    Epoch 23/100
    94/94 - 0s - loss: 0.0026 - 127ms/epoch - 1ms/step
    Epoch 24/100
    94/94 - 0s - loss: 0.0024 - 119ms/epoch - 1ms/step
    Epoch 25/100
    94/94 - 0s - loss: 0.0024 - 101ms/epoch - 1ms/step
    Epoch 26/100
    94/94 - 0s - loss: 0.0022 - 160ms/epoch - 2ms/step
    Epoch 27/100
    94/94 - 0s - loss: 0.0022 - 107ms/epoch - 1ms/step
    Epoch 28/100
    94/94 - 0s - loss: 0.0021 - 99ms/epoch - 1ms/step
    Epoch 29/100
    94/94 - 0s - loss: 0.0022 - 92ms/epoch - 979us/step
    Epoch 30/100
    94/94 - 0s - loss: 0.0021 - 97ms/epoch - 1ms/step
    Epoch 31/100
    94/94 - 0s - loss: 0.0021 - 102ms/epoch - 1ms/step
    Epoch 32/100
    94/94 - 0s - loss: 0.0021 - 94ms/epoch - 998us/step
    Epoch 33/100
    94/94 - 0s - loss: 0.0021 - 95ms/epoch - 1ms/step
    Epoch 34/100
    94/94 - 0s - loss: 0.0021 - 111ms/epoch - 1ms/step
    Epoch 35/100
    94/94 - 0s - loss: 0.0021 - 115ms/epoch - 1ms/step
    Epoch 36/100
    94/94 - 0s - loss: 0.0021 - 93ms/epoch - 988us/step
    Epoch 37/100
    94/94 - 0s - loss: 0.0021 - 91ms/epoch - 971us/step
    Epoch 38/100
    94/94 - 0s - loss: 0.0021 - 96ms/epoch - 1ms/step
    Epoch 39/100
    94/94 - 0s - loss: 0.0021 - 111ms/epoch - 1ms/step
    Epoch 40/100
    94/94 - 0s - loss: 0.0021 - 92ms/epoch - 978us/step
    Epoch 41/100
    94/94 - 0s - loss: 0.0021 - 107ms/epoch - 1ms/step
    Epoch 42/100
    94/94 - 0s - loss: 0.0020 - 123ms/epoch - 1ms/step
    Epoch 43/100
    94/94 - 0s - loss: 0.0021 - 102ms/epoch - 1ms/step
    Epoch 44/100
    94/94 - 0s - loss: 0.0021 - 115ms/epoch - 1ms/step
    Epoch 45/100
    94/94 - 0s - loss: 0.0021 - 108ms/epoch - 1ms/step
    Epoch 46/100
    94/94 - 0s - loss: 0.0020 - 98ms/epoch - 1ms/step
    Epoch 47/100
    94/94 - 0s - loss: 0.0021 - 98ms/epoch - 1ms/step
    Epoch 48/100
    94/94 - 0s - loss: 0.0021 - 99ms/epoch - 1ms/step
    Epoch 49/100
    94/94 - 0s - loss: 0.0021 - 97ms/epoch - 1ms/step
    Epoch 50/100
    94/94 - 0s - loss: 0.0020 - 95ms/epoch - 1ms/step
    Epoch 51/100
    94/94 - 0s - loss: 0.0021 - 118ms/epoch - 1ms/step
    Epoch 52/100
    94/94 - 0s - loss: 0.0021 - 126ms/epoch - 1ms/step
    Epoch 53/100
    94/94 - 0s - loss: 0.0020 - 104ms/epoch - 1ms/step
    Epoch 54/100
    94/94 - 0s - loss: 0.0021 - 92ms/epoch - 978us/step
    Epoch 55/100
    94/94 - 0s - loss: 0.0021 - 98ms/epoch - 1ms/step
    Epoch 56/100
    94/94 - 0s - loss: 0.0021 - 91ms/epoch - 971us/step
    Epoch 57/100
    94/94 - 0s - loss: 0.0021 - 114ms/epoch - 1ms/step
    Epoch 58/100
    94/94 - 0s - loss: 0.0020 - 110ms/epoch - 1ms/step
    Epoch 59/100
    94/94 - 0s - loss: 0.0021 - 94ms/epoch - 1000us/step
    Epoch 60/100
    94/94 - 0s - loss: 0.0020 - 97ms/epoch - 1ms/step
    Epoch 61/100
    94/94 - 0s - loss: 0.0020 - 99ms/epoch - 1ms/step
    Epoch 62/100
    94/94 - 0s - loss: 0.0021 - 104ms/epoch - 1ms/step
    Epoch 63/100
    94/94 - 0s - loss: 0.0020 - 105ms/epoch - 1ms/step
    Epoch 64/100
    94/94 - 0s - loss: 0.0021 - 103ms/epoch - 1ms/step
    Epoch 65/100
    94/94 - 0s - loss: 0.0021 - 103ms/epoch - 1ms/step
    Epoch 66/100
    94/94 - 0s - loss: 0.0020 - 104ms/epoch - 1ms/step
    Epoch 67/100
    94/94 - 0s - loss: 0.0021 - 95ms/epoch - 1ms/step
    Epoch 68/100
    94/94 - 0s - loss: 0.0021 - 97ms/epoch - 1ms/step
    Epoch 69/100
    94/94 - 0s - loss: 0.0021 - 94ms/epoch - 1ms/step
    Epoch 70/100
    94/94 - 0s - loss: 0.0020 - 93ms/epoch - 989us/step
    Epoch 71/100
    94/94 - 0s - loss: 0.0020 - 94ms/epoch - 1ms/step
    Epoch 72/100
    94/94 - 0s - loss: 0.0021 - 91ms/epoch - 966us/step
    Epoch 73/100
    94/94 - 0s - loss: 0.0021 - 105ms/epoch - 1ms/step
    Epoch 74/100
    94/94 - 0s - loss: 0.0021 - 108ms/epoch - 1ms/step
    Epoch 75/100
    94/94 - 0s - loss: 0.0021 - 110ms/epoch - 1ms/step
    Epoch 76/100
    94/94 - 0s - loss: 0.0021 - 103ms/epoch - 1ms/step
    Epoch 77/100
    94/94 - 0s - loss: 0.0020 - 94ms/epoch - 1000us/step
    Epoch 78/100
    94/94 - 0s - loss: 0.0021 - 93ms/epoch - 988us/step
    Epoch 79/100
    94/94 - 0s - loss: 0.0020 - 94ms/epoch - 1ms/step
    Epoch 80/100
    94/94 - 0s - loss: 0.0020 - 92ms/epoch - 974us/step
    Epoch 81/100
    94/94 - 0s - loss: 0.0020 - 93ms/epoch - 988us/step
    Epoch 82/100
    94/94 - 0s - loss: 0.0021 - 97ms/epoch - 1ms/step
    Epoch 83/100
    94/94 - 0s - loss: 0.0020 - 100ms/epoch - 1ms/step
    Epoch 84/100
    94/94 - 0s - loss: 0.0020 - 100ms/epoch - 1ms/step
    Epoch 85/100
    94/94 - 0s - loss: 0.0021 - 99ms/epoch - 1ms/step
    Epoch 86/100
    94/94 - 0s - loss: 0.0020 - 103ms/epoch - 1ms/step
    Epoch 87/100
    94/94 - 0s - loss: 0.0021 - 98ms/epoch - 1ms/step
    Epoch 88/100
    94/94 - 0s - loss: 0.0021 - 100ms/epoch - 1ms/step
    Epoch 89/100
    94/94 - 0s - loss: 0.0020 - 94ms/epoch - 1ms/step
    Epoch 90/100
    94/94 - 0s - loss: 0.0020 - 92ms/epoch - 976us/step
    Epoch 91/100
    94/94 - 0s - loss: 0.0020 - 91ms/epoch - 966us/step
    Epoch 92/100
    94/94 - 0s - loss: 0.0021 - 94ms/epoch - 996us/step
    Epoch 93/100
    94/94 - 0s - loss: 0.0021 - 100ms/epoch - 1ms/step
    Epoch 94/100
    94/94 - 0s - loss: 0.0020 - 103ms/epoch - 1ms/step
    Epoch 95/100
    94/94 - 0s - loss: 0.0021 - 101ms/epoch - 1ms/step
    Epoch 96/100
    94/94 - 0s - loss: 0.0020 - 104ms/epoch - 1ms/step
    Epoch 97/100
    94/94 - 0s - loss: 0.0020 - 99ms/epoch - 1ms/step
    Epoch 98/100
    94/94 - 0s - loss: 0.0020 - 100ms/epoch - 1ms/step
    Epoch 99/100
    94/94 - 0s - loss: 0.0020 - 94ms/epoch - 996us/step
    Epoch 100/100
    94/94 - 0s - loss: 0.0020 - 87ms/epoch - 926us/step
    1/3 [=========>....................] - ETA: 0s    3/3 [==============================] - 0s 1ms/step
    1/2 [==============>...............] - ETA: 0s    2/2 [==============================] - 0s 2ms/step
    1/2 [==============>...............] - ETA: 0s    2/2 [==============================] - 0s 1ms/step
    ==> [jsonpickle] Are test set predictions equal? True
    1/2 [==============>...............] - ETA: 0s    2/2 [==============================] - 0s 2ms/step
    ==> [manualtxt] Are test set predictions equal? True

    "\nimport sklearn_json as skljson\nskljson.to_json(new_scaler, './output/main01/test.json')\n\ndes_model = skljson.from_json('./output/main01/test.json')\nprint(des_model)\n"





|

.. code-block:: default
   :lineno-start: 8


    # Libraries
    import json
    import pandas
    import numpy as np
    import pandas as pd
    import tensorflow as tf
    import matplotlib.pyplot as plt

    from pathlib import Path
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import mean_squared_error

    # fix random seed for reproducibility
    tf.random.set_seed(7)

    # ---------------------------------------------------
    # Main example
    # ---------------------------------------------------
    # Copied from machine learning mastery

    # Path
    path = './data/passengers.csv'

    # Read data
    df = pd.read_csv(path,
        usecols=[1], engine='python')

    # Convert to numpy
    dataset = df.values
    dataset = dataset.astype('float32')

    # normalize the dataset
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(dataset)

    # split into train and test sets
    train_size = int(len(dataset) * 0.67)
    test_size = len(dataset) - train_size
    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
    print(len(train), len(test))

    def create_dataset(dataset, look_back=1):
        dataX, dataY = [], []
        for i in range(len(dataset)-look_back-1):
            a = dataset[i:(i+look_back), 0]
            dataX.append(a)
            dataY.append(dataset[i + look_back, 0])
        return np.array(dataX), np.array(dataY)

    # reshape into X=t and Y=t+1
    look_back = 1
    trainX, trainY = create_dataset(train, look_back)
    testX, testY = create_dataset(test, look_back)

    # reshape input to be [samples, time steps, features]
    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

    def create_model(look_back=1):
        """"""
        # create and fit the LSTM network
        model = Sequential()
        model.add(LSTM(4, input_shape=(1, look_back)))
        model.add(Dense(1))
        model.compile(loss='mean_squared_error', optimizer='adam')
        # Return
        return model

    # create and fit the LSTM network
    model = create_model(look_back)
    model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)

    trainPredict = model.predict(trainX)
    testPredict = model.predict(testX)

    """
    # invert predictions
    trainPredict = scaler.inverse_transform(trainPredict)
    trainY = scaler.inverse_transform([trainY])
    testPredict = scaler.inverse_transform(testPredict)
    testY = scaler.inverse_transform([testY])
    # calculate root mean squared error
    trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
    print('Train Score: %.2f RMSE' % (trainScore))
    testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
    print('Test Score: %.2f RMSE' % (testScore))

    # shift train predictions for plotting
    trainPredictPlot = np.empty_like(dataset)
    trainPredictPlot[:, :] = np.nan
    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
    # shift test predictions for plotting
    testPredictPlot = np.empty_like(dataset)
    testPredictPlot[:, :] = np.nan
    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
    # plot baseline and predictions
    plt.plot(scaler.inverse_transform(dataset))
    plt.plot(trainPredictPlot)
    plt.plot(testPredictPlot)
    #plt.show()
    """


    # ---------------------------------------
    # Save and load
    # ---------------------------------------
    # Jsonpickle
    import jsonpickle
    import jsonpickle.ext.numpy as jsonpickle_numpy

    # Configure jsonpickle
    jsonpickle.set_preferred_backend('json')
    jsonpickle_numpy.register_handlers()


    def to_jsonpickle(obj, filename):
        """"""
        with open(filename, 'w') as f:
            f.write(jsonpickle.encode(obj))


    def from_jsonpickle(filename):
        """"""
        with open(filename, 'r') as f:
            obj = jsonpickle.decode(f.read())
        return obj


    def to_txt(weights, path):
        """Converts a list of np.arrays to weights.

        .. note: The np.arrays must be 2D or less.

        Parameters
        ----------
        weights: list of np.arrays
            The weights
        path: str
            The path to save the .txt files for each layer.

        Returns
        --------
        """
        # Libraries
        from pathlib import Path
        path = Path(path)

        # Warning if more than 2D

        # Save shapes
        shapes = [e.shape for e in weights]
        with open(path / 'shapes.txt', 'w') as f:
            f.write(str(shapes))

        # Save weights
        for i, e in enumerate(weights):
            name = 'weights_layer_%s.txt' % i
            np.savetxt(path / name, e ) #, fmt='%1.100e')

    def from_txt(path):
        """Loads from a folder of txt files

        Parameters
        ----------
        path: str
            The path with the .txt files for each layer.

        Returns
        -------
        """
        import glob
        from pathlib import Path
        path = Path(path)

        # Load shapes
        with open(path / 'shapes.txt', 'r') as f:
            shapes = eval(f.read())

        # Load weights
        weights = []
        for e in glob.glob(str(path / 'weights_*')):
            weights.append(np.loadtxt(e))

        # Reshape
        reshaped = []
        for s,w in zip(shapes, weights):
            reshaped.append(np.reshape(w, s))

        # Return
        return reshaped






    # ------------------------------------------------------------
    # Example I: Serialization using jsonpickle
    # ------------------------------------------------------------
    """Description:

    The purpose of this example is to demonstrate that by 
    employing jsonpickle, we can store and retrieve the model, 
    while ensuring that the predictions for the observations 
    remain unchanged.
    """

    # Paths
    path_output_json_scaler = './output/main01/scaler.json'
    path_output_json_model = './output/main01/model.json'

    # Create path if it does not exist
    Path(path_output_json_scaler) \
        .parent.mkdir(parents=True, exist_ok=True)
    Path(path_output_json_model) \
        .parent.mkdir(parents=True, exist_ok=True)

    # Save
    to_jsonpickle(scaler, path_output_json_scaler)
    to_jsonpickle(model, path_output_json_model)

    # Load model
    jsonpickle_scaler = from_jsonpickle(path_output_json_scaler)
    jsonpickle_model = from_jsonpickle(path_output_json_model)

    # Predict
    jsonpickle_model_pred_test = jsonpickle_model.predict(testX)

    # Are the predictions equal?
    print("==> [jsonpickle] Are test set predictions equal? %s" %
        np.array_equal(testPredict, jsonpickle_model_pred_test))





    # ------------------------------------------------------------
    # Example II: Serialization using readable txt
    # ------------------------------------------------------------
    """Description:

    The purpose of this example is to demonstrate that by 
    employing a manual approach with readable txt files, we 
    can store and retrieve the model, while ensuring that 
    the predictions for the observations remain unchanged.
    """

    # Define path
    path_output_txt = './output/main01/txt'

    # Create path if it does not exist
    Path(path_output_txt).mkdir(parents=True, exist_ok=True)

    # Get weights from model
    w = jsonpickle_model.get_weights()

    # Save to txt
    to_txt(w, path=path_output_txt)

    # Load from txt
    weights = from_txt(path=path_output_txt)

    # Compare
    #for w1, a1 in zip(w, weights):
    #    print(np.array_equal(w1, a1))

    # Create model again from txt weights
    txt_model = create_model(look_back=1)
    txt_model.set_weights(weights)
    txt_model_pred_test = txt_model.predict(testX)

    # Are the predictions equal?
    print("==> [manualtxt] Are test set predictions equal? %s" %
        np.array_equal(testPredict, txt_model_pred_test))



    # ---------------------------------------
    #
    # ---------------------------------------
    # It seems that it does not work for the
    # scalers, only for a handful of models so
    # might need to do it manually too.

    """
    import sklearn_json as skljson
    skljson.to_json(new_scaler, './output/main01/test.json')

    des_model = skljson.from_json('./output/main01/test.json')
    print(des_model)
    """

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  17.189 seconds)


.. _sphx_glr_download__examples_tensorflow_plot_main_01_keras_lstm_mlm.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_main_01_keras_lstm_mlm.py <plot_main_01_keras_lstm_mlm.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_main_01_keras_lstm_mlm.ipynb <plot_main_01_keras_lstm_mlm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
